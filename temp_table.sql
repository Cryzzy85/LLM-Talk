-- phpMyAdmin SQL Dump
-- version 5.2.1
-- https://www.phpmyadmin.net/
--
-- Host: localhost
-- Erstellungszeit: 06. Jun 2023 um 23:30
-- Server-Version: 10.4.28-MariaDB
-- PHP-Version: 8.2.4

SET SQL_MODE = "NO_AUTO_VALUE_ON_ZERO";
START TRANSACTION;
SET time_zone = "+00:00";


/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8mb4 */;

--
-- Datenbank: `Paperhistory`
--

-- --------------------------------------------------------

--
-- Tabellenstruktur für Tabelle `temp_table`
--

CREATE TABLE `temp_table` (
  `Year` varchar(16) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,
  `AuthorID` mediumtext CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,
  `Title` mediumtext CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,
  `SourceID` varchar(83) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,
  `Abstract` longtext CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,
  `ResearchField` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,
  `DeploymentStatus` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT 'Pre-Deployment',
  `Publisher` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT 'OpenAI',
  `Language` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT 'English'
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci;

--
-- Daten für Tabelle `temp_table`
--

INSERT INTO `temp_table` (`Year`, `AuthorID`, `Title`, `SourceID`, `Abstract`, `ResearchField`, `DeploymentStatus`, `Publisher`, `Language`) VALUES
('2023', 'Eloundou, T.; Manning, S. et Al.  ', 'GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models', 'http://arxiv.org/abs/2303.10130', 'We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications.', 'Society and AI Responsibility', 'Post-Deployment', 'OpenAI', 'English'),
('2023', 'OpenAI', 'GPT-4 Technical Report', 'http://arxiv.org/abs/2303.08774', 'We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4\'s performance based on models trained with no more than 1/1,000th the compute of GPT-4.', 'Neural Networks and AI Modeling', 'Post-Deployment', 'OpenAI', 'English'),
('2022', 'Radford, A. & Kim, J. W. et Al. ', 'Robust Speech Recognition via Large-Scale Weak Supervision', 'https://arxiv.org/abs/2212.04356', 'We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.', 'Language Processing ', 'Pre-Deployment', 'OpenAI', 'English'),
('2022', 'Bavarian, M. & Jun, H. et Al.', 'Efficient Training of Language Models to Fill in the Middle', 'http://arxiv.org/abs/2207.14255', 'We show that autoregressive language models can learn to infill text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end. While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales. Given the usefulness, simplicity, and efficiency of training models to fill-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default. To this end, we run a series of ablations on key hyperparameters, such as the data transformation frequency, the structure of the transformation, and the method of selecting the infill span. We use these ablations to prescribe strong default settings and best practices to train FIM models. We have released our best infilling model trained with best practices in our API, and release our infilling benchmarks to aid future research.', 'Language Processing ', 'Pre-Deployment', 'OpenAI', 'English'),
('2022', 'Gao, L. & Schulman, J. et Al', 'Scaling Laws for Reward Model Overoptimization', 'http://arxiv.org/abs/2210.10760', 'In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart\'s law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed \"gold-standard\" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-$n$ sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.', 'Internal Workings and Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English'),
('2022', 'Khlaaf, H. & Mishkin, P. et Al.', 'A Hazard Analysis Framework for Code Synthesis Large Language Models', 'http://arxiv.org/abs/2207.14157', 'Codex, a large language model (LLM) trained on a variety of codebases, exceeds the previous state of the art in its capacity to synthesize and generate code. Although Codex provides a plethora of benefits, models that may generate code on such scale have significant limitations, alignment problems, the potential to be misused, and the possibility to increase the rate of progress in technical fields that may themselves have destabilizing impacts or have misuse potential. Yet such safety impacts are not yet known or remain to be explored. In this paper, we outline a hazard analysis framework constructed at OpenAI to uncover hazards or safety risks that the deployment of models like Codex may impose technically, socially, politically, and economically. The analysis is informed by a novel evaluation framework that determines the capacity of advanced code generation techniques against the complexity and expressivity of specification prompts, and their capability to understand and execute them relative to human ability.', 'Safety and AI Alignment ', 'Pre-Deployment', 'OpenAI', 'English'),
('2022', 'Lehman, J. & Gordon, J. et Al.', 'Evolution through Large Models', 'http://arxiv.org/abs/2206.08896', 'This paper pursues the insight that large language models (LLMs) trained to generate code can vastly improve the effectiveness of mutation operators applied to programs in genetic programming (GP). Because such LLMs benefit from training data that includes sequential changes and modifications, they can approximate likely changes that humans would make. To highlight the breadth of implications of such evolution through large models (ELM), in the main experiment ELM combined with MAP-Elites generates hundreds of thousands of functional examples of Python programs that output working ambulating robots in the Sodarace domain, which the original LLM had never seen in pre-training. These examples then help to bootstrap training a new conditional language model that can output the right walker for a particular terrain. The ability to bootstrap new models that can output appropriate artifacts for a given context in a domain where zero training data was previously available carries implications for open-endedness, deep learning, and reinforcement learning. These implications are explored here in depth in the hope of inspiring new directions of research now opened up by ELM.', 'Language Processing', 'Pre-Deployment', 'OpenAI', 'English'),
('2022', 'Saunders, W. & Yeh, C. et Al.', 'Self-critiquing models for assisting human evaluators', 'http://arxiv.org/abs/2206.05802', 'We fine-tune large language models to write natural language critiques (natural language critical comments) using behavioral cloning. On a topic-based summarization task, critiques written by our models help humans find flaws in summaries that they would have otherwise missed. Our models help find naturally occurring flaws in both model and human written summaries, and intentional flaws in summaries written by humans to be deliberately misleading. We study scaling properties of critiquing with both topic-based summarization and synthetic tasks. Larger models write more helpful critiques, and on most tasks, are better at self-critiquing, despite having harder-to-critique outputs. Larger models can also integrate their own self-critiques as feedback, refining their own summaries into better ones. Finally, we motivate and introduce a framework for comparing critiquing ability to generation and discrimination ability. Our measurements suggest that even large models may still have relevant knowledge they cannot or do not articulate as critiques. These results are a proof of concept for using AI-assisted human feedback to scale the supervision of machine learning systems to tasks that are difficult for humans to evaluate directly. We release our training datasets, as well as samples from our critique assistance experiments.', 'Language Processing', 'Pre-Deployment', 'OpenAI', 'English'),
('2022', 'OpenAI', 'Techniques for training large neural networks', 'https://openai.com/research/techniques-for-training-large-neural-networks', 'Large neural networks are at the core of many recent advances in AI, but training them is a difficult engineering and research challenge which requires orchestrating a cluster of GPUs to perform a single synchronized calculation.', 'Language Processing', 'Pre-Deployment', 'OpenAI', 'English'),
('2022', 'Lin, S. & Hilton, J.et Al.', 'Teaching Models to Express Their Uncertainty in Words', 'http://arxiv.org/abs/2205.14334', 'We show that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits. When given a question, the model generates both an answer and a level of confidence (e.g. \"90% confidence\" or \"high confidence\"). These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift, and is sensitive to uncertainty in its own answers, rather than imitating human examples. To our knowledge, this is the first time a model has been shown to express calibrated uncertainty about its own answers in natural language. For testing calibration, we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words (\"verbalized probability\") to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT-3\'s ability to generalize calibration depends on pre-trained latent representations that correlate with epistemic uncertainty over its answers.', 'Internal Workings and Training Methodologies', 'Pre-Deployment', 'OpenAI', 'English'),
('2022', 'OpenAI', 'Measuring Goodhart’s law', 'https://openai.com/research/measuring-goodharts-law', 'Goodhart’s law famously says: “When a measure becomes a target, it ceases to be a good measure.” Although originally from economics, it’s something we have to grapple with at OpenAI when figuring out how to optimize objectives that are difficult or costly to measure.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English'),
('2022', 'OpenAI', 'Lessons learned on language model safety and misuse', 'https://openai.com/research/language-model-safety-and-misuse', 'We describe our latest thinking in the hope of helping other AI developers address safety and misuse of deployed models.', 'Safety and AI Alignment ', 'Pre-Deployment', 'OpenAI', 'English'),
('2022', 'Polu, S. & Han, J. et Al.', 'Formal Mathematics Statement Curriculum Learning', 'http://arxiv.org/abs/2202.01344', 'We explore the use of expert iteration in the context of language modeling applied to formal mathematics. We show that at same compute budget, expert iteration, by which we mean proof search interleaved with learning, dramatically outperforms proof search only. We also observe that when applied to a collection of formal statements of sufficiently varied difficulty, expert iteration is capable of finding and solving a curriculum of increasingly difficult problems, without the need for associated ground-truth proofs. Finally, by applying this expert iteration to a manually curated set of problem statements, we achieve state-of-the-art on the miniF2F benchmark, automatically solving multiple challenging problems drawn from high school olympiads.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English'),
('2022', 'Ouyang, L. & Wu, J. et Al.', 'Training language models to follow instructions with human feedback', 'http://arxiv.org/abs/2203.02155', 'Making language models bigger does not inherently make them better at following a user\'s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.', 'Safety and AI Alignment ', 'Pre-Deployment', 'OpenAI', 'English'),
('2022', 'Neelakantan, A. & Xu, T. et Al.', 'Text and Code Embeddings by Contrastive Pre-Training', 'http://arxiv.org/abs/2201.10005', 'Text embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4% and 1.8% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4%, 14.7%, and 10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8% relative improvement over prior best work on code search.', 'Language Processing ', 'Pre-Deployment', 'OpenAI', 'English'),
('2022', 'Nakano, R. & Hilton, J. et Al.', 'WebGPT: Browser-assisted question-answering with human feedback', 'http://arxiv.org/abs/2112.09332', 'We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model\'s answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.', 'Language Processing ', 'Pre-Deployment', 'OpenAI', 'English'),
('2021', 'Cobbe, K. & Kosaraju, V. et Al.', 'Training Verifiers to Solve Math Word Problems', 'http://arxiv.org/abs/2110.14168', 'State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.', 'Language Processing ', 'Pre-Deployment', 'OpenAI', 'English'),
('2022', 'Lin, S. & Hilton, J.et Al.', 'TruthfulQA: Measuring How Models Mimic Human Falsehoods', 'http://arxiv.org/abs/2109.07958', 'We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.', 'Safety and AI Alignment ', 'Pre-Deployment', 'OpenAI', 'English'),
('2021', 'Chen, M. & Tworek, J. et Al.', 'Evaluating Large Language Models Trained on Code', 'http://arxiv.org/abs/2107.03374', 'We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.', 'Language Processing ', 'Pre-Deployment', 'OpenAI', 'English'),
('2021', 'Tamkin, A. & Brundage, M. et Al.', 'Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models', 'http://arxiv.org/abs/2102.02503', 'On October 14th, 2020, researchers from OpenAI, the Stanford Institute for Human-Centered Artificial Intelligence, and other universities convened to discuss open research questions surrounding GPT-3, the largest publicly-disclosed dense language model at the time. The meeting took place under Chatham House Rules. Discussants came from a variety of research backgrounds including computer science, linguistics, philosophy, political science, communications, cyber policy, and more. Broadly, the discussion centered around two main questions: 1) What are the technical capabilities and limitations of large language models? 2) What are the societal effects of widespread use of large language models? Here, we provide a detailed summary of the discussion organized by the two themes above.', 'Society and AI Responsibility', 'Pre-Deployment', 'OpenAI', 'English'),
('2020', 'Polu, S. & Sutskever, I. et Al.', 'Generative Language Modeling for Automated Theorem Proving', 'http://arxiv.org/abs/2009.03393', 'We explore the application of transformer-based language models to automated theorem proving. This work is motivated by the possibility that a major limitation of automated theorem provers compared to humans -- the generation of original mathematical terms -- might be addressable via generation from language models. We present an automated prover and proof assistant, GPT-f, for the Metamath formalization language, and analyze its performance. GPT-f found new short proofs that were accepted into the main Metamath library, which is to our knowledge, the first time a deep-learning based system has contributed proofs that were adopted by a formal mathematics community.', 'Language Processing ', 'Pre-Deployment', 'OpenAI', 'English'),
('2022', 'Stiennon, N. & Ouyang, L. et Al.', 'Learning to summarize from human feedback', 'http://arxiv.org/abs/2009.01325', 'As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.', 'Language Processing ', 'Pre-Deployment', 'OpenAI', 'English'),
('2020', 'Brown, T. B. & Mann, B. et Al.', 'Language Models are Few-Shot Learners', 'http://arxiv.org/abs/2005.14165', 'Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3\'s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.', 'Internal Workings and Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English'),
('2020', 'Kaplan, J. & McCandlish, S. et Al.', 'Scaling Laws for Neural Language Models', 'http://arxiv.org/abs/2001.08361', 'We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English'),
('2019', 'Solaiman, I. &  Brundage, Miles; C. et Al.', 'Release Strategies and the Social Impacts of Language Models', 'http://arxiv.org/abs/1908.09203', 'Large language models have a range of beneficial uses: they can assist in prose, poetry, and programming; analyze dataset biases; and more. However, their flexibility and generative capabilities also raise misuse concerns. This report discusses OpenAI\'s work related to the release of its GPT-2 language model. It discusses staged release, which allows time between model releases to conduct risk and benefit analyses as model sizes increased. It also discusses ongoing partnership-based research and provides recommendations for better coordination and responsible publication in AI.', 'Society and AI Responsibility', 'Pre-Deployment', 'OpenAI', 'English'),
('2020', 'Ziegler, D. M. & Stiennon, N. et Al.', 'Fine-Tuning Language Models from Human Preferences', 'http://arxiv.org/abs/1909.08593', 'Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.', 'Internal Workings and Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English'),
('2020', 'Baker, B. & Kanitscheider, I. et Al.', 'Emergent Tool Use From Multi-Agent Autocurricula', 'http://arxiv.org/abs/1909.07528', 'Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.', 'Internal Workings and Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English'),
('2019', 'Child, R. & Gray, S. et Al.', 'Generating Long Sequences with Sparse Transformers', 'http://arxiv.org/abs/1904.10509', 'Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \\sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English'),
('2020', 'Du, Y. & Mordatch, I. et Al.', 'Implicit Generation and Generalization in Energy-Based Models', 'http://arxiv.org/abs/1903.08689', 'Energy based models (EBMs) are appealing due to their generality and simplicity in likelihood modeling, but have been traditionally difficult to train. We present techniques to scale MCMC based EBM training on continuous neural networks, and we show its success on the high-dimensional data domains of ImageNet32x32, ImageNet128x128, CIFAR-10, and robotic hand trajectories, achieving better samples than other likelihood models and nearing the performance of contemporary GAN approaches, while covering all modes of the data. We highlight some unique capabilities of implicit generation such as compositionality and corrupt image reconstruction and inpainting. Finally, we show that EBMs are useful models across a wide variety of tasks, achieving state-of-the-art out-of-distribution classification, adversarially robust classification, state-of-the-art continual online class learning, and coherent long term predicted trajectory rollouts.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English'),
('2019', 'Irving, G. & Askell, A. et Al.', 'AI Safety Needs Social Scientists', 'https://distill.pub/2019/safety-needs-social-scientists', 'If we want to train AI to do what humans want, we need to study humans.', 'Society and AI Responsibility', 'Pre-Deployment', 'OpenAI', 'English'),
('2019', 'Lowrey, K. & Rajeswaran, A. et Al.', 'Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control', 'http://arxiv.org/abs/1811.01848', 'We propose a plan online and learn offline (POLO) framework for the setting where an agent, with an internal model, needs to continually act and learn in the world. Our work builds on the synergistic relationship between local model-based control, global value function learning, and exploration. We study how local trajectory optimization can cope with approximation errors in the value function, and can stabilize and accelerate value function learning. Conversely, we also study how approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions. Finally, we also demonstrate how trajectory optimization can be used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation. This exploration is critical for fast and stable learning of the value function. Combining these components enable solutions to complex simulated control tasks, like humanoid locomotion and dexterous in-hand manipulation, in the equivalent of a few minutes of experience in the real world.', 'Internal Workings and Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English'),
('2018', 'Burda, Y. & Edwards, H. et A.', 'Exploration by Random Network Distillation', 'http://arxiv.org/abs/1810.12894', 'We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma\'s Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.', 'Internal Workings and Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English'),
('2018', 'Christiano, P. & Shlegeris, B. et Al.', 'Supervising strong learners by amplifying weak experts', 'http://arxiv.org/abs/1810.08575', 'Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.', 'Internal Workings and Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English'),
('2018', 'Grathwohl, W. & Chen, R. T. Q. et Al.', 'FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models', 'http://arxiv.org/abs/1810.01367', 'A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson\'s trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.', 'Internal Workings and Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English'),
('2018', 'Burda, Y. & Edwards, H. et Al.', 'Large-Scale Study of Curiosity-Driven Learning', 'http://arxiv.org/abs/1808.04355', 'Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github.io/large-scale-curiosity/', 'Internal Workings and Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English'),
('2018', 'OpenAI', 'Learning Montezuma’s Revenge from a single demonstration', 'https://openai.com/research/learning-montezumas-revenge-from-a-single-demonstration', 'We’ve trained an agent to achieve a high score of 74,500 on Montezuma’s Revenge from a single human demonstration, better than any previously published result. Our algorithm is simple: the agent plays a sequence of games starting from carefully chosen states from the demonstration, and learns from them by optimizing the game score using PPO, the same reinforcement learning algorithm that underpins OpenAI Five.', 'Internal Workings and Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English'),
('2018', 'Grover, A. & Al-Shedivat, M. et Al.', 'Learning Policy Representations in Multiagent Systems', 'http://arxiv.org/abs/1806.06464', 'Modeling agent behavior is central to understanding the emergence of complex phenomena in multiagent systems. Prior work in agent modeling has largely been task-specific and driven by hand-engineering domain-specific prior knowledge. We propose a general learning framework for modeling agent behavior in any multiagent system using only a handful of interaction data. Our framework casts agent modeling as a representation learning problem. Consequently, we construct a novel objective inspired by imitation learning and agent identification and design an algorithm for unsupervised learning of representations of agent policies. We demonstrate empirically the utility of the proposed framework in (i) a challenging high-dimensional competitive environment for continuous control and (ii) a cooperative environment for communication, on supervised predictive tasks, unsupervised clustering, and policy optimization using deep reinforcement learning.', 'Internal Workings and Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English'),
('2018', 'Irving, G. & Christiano, P. et Al.', 'AI safety via debate', 'http://arxiv.org/abs/1805.00899', 'To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier\'s accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.', 'Safety and AI Alignment ', 'Pre-Deployment', 'OpenAI', 'English'),
('2018', 'Houthooft, R. & Chen, R. et Al.', 'Evolved Policy Gradients', 'http://arxiv.org/abs/1802.04821', 'We propose a metalearning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss is parametrized via temporal convolutions over the agent\'s experience. Because this loss is highly flexible in its ability to take into account the agent\'s history, it enables fast task learning. Empirical results show that our evolved policy gradient algorithm (EPG) achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method. We also demonstrate that EPG\'s learned loss can generalize to out-of-distribution test time tasks, and exhibits qualitatively different behavior from other popular metalearning algorithms.', 'Internal Workings and Training Methodologies', 'Pre-Deployment', 'OpenAI', 'English'),
('2018', 'Wu, C. & Rajeswaran, A. et Al.', 'Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines', 'http://arxiv.org/abs/1803.07246', 'Policy gradient methods have enjoyed great success in deep reinforcement learning but suffer from high variance of gradient estimates. The high variance problem is particularly exasperated in problems with long horizons or high-dimensional action spaces. To mitigate this issue, we derive a bias-free action-dependent baseline for variance reduction which fully exploits the structural form of the stochastic policy itself and does not make any additional assumptions about the MDP. We demonstrate and quantify the benefit of the action-dependent baseline through both theoretical analysis as well as numerical results, including an analysis of the suboptimality of the optimal state-dependent baseline. The result is a computationally efficient policy gradient algorithm, which scales to high-dimensional control problems, as demonstrated by a synthetic 2000-dimensional target matching task. Our experimental results indicate that action-dependent baselines allow for faster learning on standard reinforcement learning benchmarks and high-dimensional hand manipulation and synthetic tasks. Finally, we show that the general idea of including additional information in baselines for improved variance reduction can be extended to partially observed and multi-agent tasks.', 'Internal Workings and Training Methodologies', 'Pre-Deployment', 'OpenAI', 'English'),
('2018', 'Nichol, A. & Achiam, J. et Al.', 'On First-Order Meta-Learning Algorithms', 'http://arxiv.org/abs/1803.02999', 'This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.', 'Internal Workings and Training Methodologies', 'Pre-Deployment', 'OpenAI', 'English'),
('2019', 'Stadie, B. & Yang, G. et Al.', 'Some Considerations on Learning to Explore via Meta-Reinforcement Learning', 'http://arxiv.org/abs/1803.01118', 'We consider the problem of exploration in meta reinforcement learning. Two new meta reinforcement learning algorithms are suggested: E-MAML and E-$\\text{RL}^2$. Results are presented on a novel environment we call `Krazy World\' and a set of maze environments. We show E-MAML and E-$\\text{RL}^2$ deliver better performance on tasks where exploration is important.', 'Internal Workings and Training Methodologies', 'Pre-Deployment', 'OpenAI', 'English'),
('2018', 'Brundage, M. & Avin, S. et Al.', 'The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation', 'http://arxiv.org/abs/1802.07228', 'This report surveys the landscape of potential security threats from malicious uses of AI, and proposes ways to better forecast, prevent, and mitigate these threats. After analyzing the ways in which AI may influence the threat landscape in the digital, physical, and political domains, we make four high-level recommendations for AI researchers and other stakeholders. We also suggest several promising areas for further research that could expand the portfolio of defenses, or make attacks less effective or harder to execute. Finally, we discuss, but do not conclusively resolve, the long-term equilibrium of attackers and defenders.', 'Safety and AI Alignment', 'Pre-Deployment', 'OpenAI', 'English');
INSERT INTO `temp_table` (`Year`, `AuthorID`, `Title`, `SourceID`, `Abstract`, `ResearchField`, `DeploymentStatus`, `Publisher`, `Language`) VALUES
('2018', 'Milli, S. & Abbeel, P. et Al.', 'Interpretable and Pedagogical Examples', 'http://arxiv.org/abs/1711.00694', 'Teachers intentionally pick the most informative examples to show their students. However, if the teacher and student are neural networks, the examples that the teacher network learns to give, although effective at teaching the student, are typically uninterpretable. We show that training the student and teacher iteratively, rather than jointly, can produce interpretable teaching strategies. We evaluate interpretability by (1) measuring the similarity of the teacher\'s emergent strategies to intuitive strategies in each domain and (2) conducting human experiments to evaluate how effective the teacher\'s strategies are at teaching humans. We show that the teacher network learns to select or generate interpretable, pedagogical examples to teach rule-based, probabilistic, boolean, and hierarchical concepts.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English'),
('2018', 'Raiman, J. & Raiman, O. et Al.', 'DeepType: Multilingual Entity Linking by Neural Type System Evolution', 'http://arxiv.org/abs/1802.01021', 'The wealth of structured (e.g. Wikidata) and unstructured data about the world available today presents an incredible opportunity for tomorrow\'s Artificial Intelligence. So far, integration of these two different modalities is a difficult process, involving many decisions concerning how best to represent the information so that it will be captured or useful, and hand-labeling large amounts of data. DeepType overcomes this challenge by explicitly integrating symbolic information into the reasoning process of a neural network with a type system. First we construct a type system, and second, we use it to constrain the outputs of a neural network to respect the symbolic structure. We achieve this by reformulating the design problem into a mixed integer problem: create a type system and subsequently train a neural network with it. In this reformulation discrete variables select which parent-child relations from an ontology are types within the type system, while continuous variables control a classifier fit to the type system. The original problem cannot be solved exactly, so we propose a 2-step algorithm: 1) heuristic search or stochastic optimization over discrete variables that define a type system informed by an Oracle and a Learnability heuristic, 2) gradient descent to fit classifier parameters. We apply DeepType to the problem of Entity Linking on three standard datasets (i.e. WikiDisamb30, CoNLL (YAGO), TAC KBP 2010) and find that it outperforms all existing solutions by a wide margin, including approaches that rely on a human-designed type system or recent deep learning-based entity embeddings, while explicitly using symbolic information lets it integrate new entities without retraining.', 'Internal Workings and Training Methodologies', 'Pre-Deployment', 'OpenAI', 'English'),
('2018', 'Louizos, C. & Welling, M. et Al. ', 'Learning Sparse Neural Networks through Regularization', 'http://arxiv.org/abs/1712.01312', 'We propose a practical method for L0 norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of L0 regularization. However, since the L0 norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected L0 regularized objective is differentiable with respect to the distribution parameters. We further propose the hard concrete distribution for the gates, which is obtained by “stretching” a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efﬁcient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English'),
('2017', 'Frans, K. & Ho, J. et Al.', 'Meta Learning Shared Hierarchies', 'http://arxiv.org/abs/1710.09767', 'We develop a metalearning approach for learning hierarchically structured policies, improving sample efficiency on unseen tasks through the use of shared primitives---policies that are executed for large numbers of timesteps. Specifically, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.', 'Internal Workings and Training Methodologies', 'Pre-Deployment', 'OpenAI', 'English'),
('2018', 'Foerster, J. & Chen, R. et Al.', 'Learning with Opponent-Learning Awareness', 'http://arxiv.org/abs/1709.04326', 'Multi-agent settings are quickly gathering importance in machine learning. This includes a plethora of recent work on deep multi-agent reinforcement learning, but also can be extended to hierarchical RL, generative adversarial networks and decentralised optimisation. In all these settings the presence of multiple learning agents renders the training problem non-stationary and often leads to unstable training or undesired final results. We present Learning with Opponent-Learning Awareness (LOLA), a method in which each agent shapes the anticipated learning of the other agents in the environment. The LOLA learning rule includes a term that accounts for the impact of one agent\'s policy on the anticipated parameter update of the other agents. Results show that the encounter of two LOLA agents leads to the emergence of tit-for-tat and therefore cooperation in the iterated prisoners\' dilemma, while independent learning does not. In this domain, LOLA also receives higher payouts compared to a naive learner, and is robust against exploitation by higher order gradient-based methods. Applied to repeated matching pennies, LOLA agents converge to the Nash equilibrium. In a round robin tournament we show that LOLA agents successfully shape the learning of a range of multi-agent learning algorithms from literature, resulting in the highest average returns on the IPD. We also show that the LOLA update rule can be efficiently calculated using an extension of the policy gradient estimator, making the method suitable for model-free RL. The method thus scales to large parameter and input spaces and nonlinear function approximators. We apply LOLA to a grid world task with an embedded social dilemma using recurrent policies and opponent modelling. By explicitly considering the learning of the other agent, LOLA agents learn to cooperate out of self-interest. The code is at github.com/alshedivat/lola.', 'Internal Workings and Training Methodologies', 'Pre-Deployment', 'OpenAI', 'English'),
('2017', 'Matiisen, T. & Oliver, A. et Al.', 'Teacher-Student Curriculum Learning', 'http://arxiv.org/abs/1707.00183', 'We propose Teacher-Student Curriculum Learning (TSCL), a framework for automatic curriculum learning, where the Student tries to learn a complex task and the Teacher automatically chooses subtasks from a given set for the Student to train on. We describe a family of Teacher algorithms that rely on the intuition that the Student should practice more those tasks on which it makes the fastest progress, i.e. where the slope of the learning curve is highest. In addition, the Teacher algorithms address the problem of forgetting by also choosing tasks where the Student\'s performance is getting worse. We demonstrate that TSCL matches or surpasses the results of carefully hand-crafted curricula in two tasks: addition of decimal numbers with LSTM and navigation in Minecraft. Using our automatically generated curriculum enabled to solve a Minecraft maze that could not be solved at all when training directly on solving the maze, and the learning was an order of magnitude faster than uniform sampling of subtasks.', 'Internal Workings and Training Methodologies', 'Pre-Deployment', 'OpenAI', 'English'),
('2023', 'Christiano, P. & Leike, J. et Al.', 'Deep reinforcement learning from human preferences', 'http://arxiv.org/abs/1706.03741', 'For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent\'s interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.', 'Internal Workings and Training Methodologies', 'Post-Deployment', 'OpenAI', 'English'),
('2020', 'Lowe, R. & Wu, Y. et Al.', 'Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments', 'http://arxiv.org/abs/1706.02275', 'We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.', 'Internal Workings and Training Methodologies', 'Pre-Deployment', 'OpenAI', 'English'),
('2017', 'Florensa, C. & Duan, Y. et Al.', 'Stochastic Neural Networks for Hierarchical Reinforcement Learning', 'http://arxiv.org/abs/1704.03012', 'Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks. To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.', 'Internal Workings and Training Methodologies', 'Pre-Deployment', 'OpenAI', 'English'),
('2017', 'Salimans, T. & Ho, J. et Al.', 'Evolution Strategies as a Scalable Alternative to Reinforcement Learning', 'http://arxiv.org/abs/1703.03864', 'We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.', 'Internal Workings and Training Methodologies', 'Pre-Deployment', 'OpenAI', 'English'),
('2017', 'OpenAI', 'Learning to communicate', 'https://openai.com/research/learning-to-communicate', 'In this post we’ll outline new OpenAI research in which agents develop their own language.', 'Language Processing ', 'Pre-Deployment', 'OpenAI', 'English'),
('2018', 'Mordatch, I. & Abbeel, P. et Al.', 'Emergence of Grounded Compositional Language in Multi-Agent Populations', 'http://arxiv.org/abs/1703.04908', 'By capturing statistical patterns in large corpora, machine learning has enabled significant advances in natural language processing, including in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply capturing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. Towards this end, we propose a multi-agent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of non-verbal communication such as pointing and guiding when language communication is unavailable.', 'Language Processing ', 'Pre-Deployment', 'OpenAI', 'English'),
('2017', 'Huang, S. & Papernot, N. et Al.', 'Adversarial Attacks on Neural Network Policies', 'http://arxiv.org/abs/1702.02284', 'Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial.', 'Safety and AI Alignment ', 'Pre-Deployment', 'OpenAI', 'English'),
('2017', ' Tang, Haoran & Houthooft, Rein et. Al. ', '#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning', 'http://arxiv.org/abs/1611.04717', 'Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.', 'Safety and AI Alignment ', 'Pre-Deployment', 'OpenAI', 'English'),
('2016', 'Price, E. & Zaremba, W. et. Al. ', 'Extensions and Limitations of the Neural GPU', 'http://arxiv.org/abs/1611.00736', 'The Neural GPU is a recent model that can learn algorithms such as multi-digit binary addition and binary multiplication in a way that generalizes to inputs of arbitrary length. We show that there are two simple ways of improving the performance of the Neural GPU: by carefully designing a curriculum, and by increasing model size. The latter requires a memory efficient implementation, as a naive implementation of the Neural GPU is memory intensive. We find that these techniques increase the set of algorithmic problems that can be solved by the Neural GPU: we have been able to learn to perform all the arithmetic operations (and generalize to arbitrarily long numbers) when the arguments are given in the decimal representation (which, surprisingly, has not been possible before). We have also been able to train the Neural GPU to evaluate long arithmetic expressions with multiple operands that require respecting the precedence order of the operands, although these have succeeded only in their binary representation, and not with perfect accuracy. In addition, we gain insight into the Neural GPU by investigating its failure modes. We find that Neural GPUs that correctly generalize to arbitrarily long numbers still fail to compute the correct answer on highly-symmetric, atypical inputs: for example, a Neural GPU that achieves near-perfect generalization on decimal multiplication of up to 100-digit long numbers can fail on $000000\\dots002 \\times 000000\\dots002$ while succeeding at $2 \\times 2$. These failure modes are reminiscent of adversarial examples.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English'),
('2016', 'OpenAI', 'Generative models', 'https://openai.com/research/generative-models', 'This post describes four projects that share a common theme of enhancing or using generative models, a branch of unsupervised learning techniques in machine learning. In addition to describing our work, this post will tell you a bit more about generative models: what they are, why they are important, and where they might be going.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English'),
('2016', 'OpenAI', 'Infrastructure for deep learning', 'https://openai.com/research/infrastructure-for-deep-learning', 'Deep learning is an empirical science, and the quality of a group’s infrastructure is a multiplier on progress. Fortunately, today’s open-source ecosystem makes it possible for anyone to build great deep learning infrastructure.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English'),
('2021', 'Miyato, T. & Dai, A. M. et. Al. ', 'Adversarial Training Methods for Semi-Supervised Text Classification', 'http://arxiv.org/abs/1605.07725', 'Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting. Code is available at https://github.com/tensorflow/models/tree/master/research/adversarial_text.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English'),
('2016', 'Salimans, T. & Kingma, D. et. Al.', 'Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks', 'http://arxiv.org/abs/1602.07868', 'We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English');
COMMIT;

/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
