-- phpMyAdmin SQL Dump
-- version 5.2.1
-- https://www.phpmyadmin.net/
--
-- Host: localhost
-- Erstellungszeit: 10. Jun 2023 um 23:02
-- Server-Version: 10.4.28-MariaDB
-- PHP-Version: 8.2.4

SET SQL_MODE = "NO_AUTO_VALUE_ON_ZERO";
START TRANSACTION;
SET time_zone = "+00:00";


/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8mb4 */;

--
-- Datenbank: `Paperhistory`
--

-- --------------------------------------------------------

--
-- Tabellenstruktur für Tabelle `CoCreating`
--

CREATE TABLE `CoCreating` (
  `Year` varchar(16) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,
  `AuthorID` mediumtext CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,
  `Title` mediumtext CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,
  `SourceID` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `Abstract` longtext CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,
  `ResearchField` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,
  `DeploymentStatus` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT 'Pre-Deployment',
  `Publisher` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT 'OpenAI',
  `Language` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT 'English',
  `ComplexityState` varchar(255) DEFAULT NULL,
  `EvolvementRealm` varchar(255) DEFAULT NULL,
  `StartingPoint` varchar(255) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci;

--
-- Daten für Tabelle `CoCreating`
--

INSERT INTO `CoCreating` (`Year`, `AuthorID`, `Title`, `SourceID`, `Abstract`, `ResearchField`, `DeploymentStatus`, `Publisher`, `Language`, `ComplexityState`, `EvolvementRealm`, `StartingPoint`) VALUES
('2016', 'Salimans, T. & Kingma, D. et. Al.', 'Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks', 'http://arxiv.org/abs/1602.07868', 'We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'In-House provided'),
('2021', 'Miyato, T. & Dai, A. M. et. Al. ', 'Adversarial Training Methods for Semi-Supervised Text Classification', 'http://arxiv.org/abs/1605.07725', 'Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting. Code is available at https://github.com/tensorflow/models/tree/master/research/adversarial_text.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2016', 'Price, E. & Zaremba, W. et. Al. ', 'Extensions and Limitations of the Neural GPU', 'http://arxiv.org/abs/1611.00736', 'The Neural GPU is a recent model that can learn algorithms such as multi-digit binary addition and binary multiplication in a way that generalizes to inputs of arbitrary length. We show that there are two simple ways of improving the performance of the Neural GPU: by carefully designing a curriculum, and by increasing model size. The latter requires a memory efficient implementation, as a naive implementation of the Neural GPU is memory intensive. We find that these techniques increase the set of algorithmic problems that can be solved by the Neural GPU: we have been able to learn to perform all the arithmetic operations (and generalize to arbitrarily long numbers) when the arguments are given in the decimal representation (which, surprisingly, has not been possible before). We have also been able to train the Neural GPU to evaluate long arithmetic expressions with multiple operands that require respecting the precedence order of the operands, although these have succeeded only in their binary representation, and not with perfect accuracy. In addition, we gain insight into the Neural GPU by investigating its failure modes. We find that Neural GPUs that correctly generalize to arbitrarily long numbers still fail to compute the correct answer on highly-symmetric, atypical inputs: for example, a Neural GPU that achieves near-perfect generalization on decimal multiplication of up to 100-digit long numbers can fail on $000000\\dots002 \\times 000000\\dots002$ while succeeding at $2 \\times 2$. These failure modes are reminiscent of adversarial examples.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'In-House provided'),
('2017', ' Tang, Haoran & Houthooft, Rein et. Al. ', '#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning', 'http://arxiv.org/abs/1611.04717', 'Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.', 'Society and AI Alignment', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'In-House provided'),
('2017', 'Huang, S. & Papernot, N. et Al.', 'Adversarial Attacks on Neural Network Policies', 'http://arxiv.org/abs/1702.02284', 'Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial.', 'Society and AI Alignment', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'In-House provided'),
('2017', 'Salimans, T. & Ho, J. et Al.', 'Evolution Strategies as a Scalable Alternative to Reinforcement Learning', 'http://arxiv.org/abs/1703.03864', 'We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'In-House provided'),
('2018', 'Mordatch, I. & Abbeel, P. et Al.', 'Emergence of Grounded Compositional Language in Multi-Agent Populations', 'http://arxiv.org/abs/1703.04908', 'By capturing statistical patterns in large corpora, machine learning has enabled significant advances in natural language processing, including in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply capturing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. Towards this end, we propose a multi-agent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of non-verbal communication such as pointing and guiding when language communication is unavailable.', 'NLP', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Core: Language Processing', 'In-House provided'),
('2017', 'Florensa, C. & Duan, Y. et Al.', 'Stochastic Neural Networks for Hierarchical Reinforcement Learning', 'http://arxiv.org/abs/1704.03012', 'Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks. To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'In-House provided'),
('2020', 'Lowe, R. & Wu, Y. et Al.', 'Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments', 'http://arxiv.org/abs/1706.02275', 'We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'In-House provided'),
('2023', 'Christiano, P. & Leike, J. et Al.', 'Deep reinforcement learning from human preferences', 'http://arxiv.org/abs/1706.03741', 'For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent\'s interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.', 'Training Methodologies ', 'Post-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2017', 'Matiisen, T. & Oliver, A. et Al.', 'Teacher-Student Curriculum Learning', 'http://arxiv.org/abs/1707.00183', 'We propose Teacher-Student Curriculum Learning (TSCL), a framework for automatic curriculum learning, where the Student tries to learn a complex task and the Teacher automatically chooses subtasks from a given set for the Student to train on. We describe a family of Teacher algorithms that rely on the intuition that the Student should practice more those tasks on which it makes the fastest progress, i.e. where the slope of the learning curve is highest. In addition, the Teacher algorithms address the problem of forgetting by also choosing tasks where the Student\'s performance is getting worse. We demonstrate that TSCL matches or surpasses the results of carefully hand-crafted curricula in two tasks: addition of decimal numbers with LSTM and navigation in Minecraft. Using our automatically generated curriculum enabled to solve a Minecraft maze that could not be solved at all when training directly on solving the maze, and the learning was an order of magnitude faster than uniform sampling of subtasks.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'In-House provided'),
('2018', 'Foerster, J. & Chen, R. et Al.', 'Learning with Opponent-Learning Awareness', 'http://arxiv.org/abs/1709.04326', 'Multi-agent settings are quickly gathering importance in machine learning. This includes a plethora of recent work on deep multi-agent reinforcement learning, but also can be extended to hierarchical RL, generative adversarial networks and decentralised optimisation. In all these settings the presence of multiple learning agents renders the training problem non-stationary and often leads to unstable training or undesired final results. We present Learning with Opponent-Learning Awareness (LOLA), a method in which each agent shapes the anticipated learning of the other agents in the environment. The LOLA learning rule includes a term that accounts for the impact of one agent\'s policy on the anticipated parameter update of the other agents. Results show that the encounter of two LOLA agents leads to the emergence of tit-for-tat and therefore cooperation in the iterated prisoners\' dilemma, while independent learning does not. In this domain, LOLA also receives higher payouts compared to a naive learner, and is robust against exploitation by higher order gradient-based methods. Applied to repeated matching pennies, LOLA agents converge to the Nash equilibrium. In a round robin tournament we show that LOLA agents successfully shape the learning of a range of multi-agent learning algorithms from literature, resulting in the highest average returns on the IPD. We also show that the LOLA update rule can be efficiently calculated using an extension of the policy gradient estimator, making the method suitable for model-free RL. The method thus scales to large parameter and input spaces and nonlinear function approximators. We apply LOLA to a grid world task with an embedded social dilemma using recurrent policies and opponent modelling. By explicitly considering the learning of the other agent, LOLA agents learn to cooperate out of self-interest. The code is at github.com/alshedivat/lola.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'In-House provided'),
('2017', 'Frans, K. & Ho, J. et Al.', 'Meta Learning Shared Hierarchies', 'http://arxiv.org/abs/1710.09767', 'We develop a metalearning approach for learning hierarchically structured policies, improving sample efficiency on unseen tasks through the use of shared primitives---policies that are executed for large numbers of timesteps. Specifically, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'In-House provided'),
('2018', 'Milli, S. & Abbeel, P. et Al.', 'Interpretable and Pedagogical Examples', 'http://arxiv.org/abs/1711.00694', 'Teachers intentionally pick the most informative examples to show their students. However, if the teacher and student are neural networks, the examples that the teacher network learns to give, although effective at teaching the student, are typically uninterpretable. We show that training the student and teacher iteratively, rather than jointly, can produce interpretable teaching strategies. We evaluate interpretability by (1) measuring the similarity of the teacher\'s emergent strategies to intuitive strategies in each domain and (2) conducting human experiments to evaluate how effective the teacher\'s strategies are at teaching humans. We show that the teacher network learns to select or generate interpretable, pedagogical examples to teach rule-based, probabilistic, boolean, and hierarchical concepts.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Core: Language Processing\n\n', 'In-House provided'),
('2018', 'Louizos, C. & Welling, M. et Al. ', 'Learning Sparse Neural Networks through Regularization', 'http://arxiv.org/abs/1712.01312', 'We propose a practical method for L0 norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of L0 regularization. However, since the L0 norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected L0 regularized objective is differentiable with respect to the distribution parameters. We further propose the hard concrete distribution for the gates, which is obtained by “stretching” a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efﬁcient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'In-House provided'),
('2018', 'Raiman, J. & Raiman, O. et Al.', 'DeepType: Multilingual Entity Linking by Neural Type System Evolution', 'http://arxiv.org/abs/1802.01021', 'The wealth of structured (e.g. Wikidata) and unstructured data about the world available today presents an incredible opportunity for tomorrow\'s Artificial Intelligence. So far, integration of these two different modalities is a difficult process, involving many decisions concerning how best to represent the information so that it will be captured or useful, and hand-labeling large amounts of data. DeepType overcomes this challenge by explicitly integrating symbolic information into the reasoning process of a neural network with a type system. First we construct a type system, and second, we use it to constrain the outputs of a neural network to respect the symbolic structure. We achieve this by reformulating the design problem into a mixed integer problem: create a type system and subsequently train a neural network with it. In this reformulation discrete variables select which parent-child relations from an ontology are types within the type system, while continuous variables control a classifier fit to the type system. The original problem cannot be solved exactly, so we propose a 2-step algorithm: 1) heuristic search or stochastic optimization over discrete variables that define a type system informed by an Oracle and a Learnability heuristic, 2) gradient descent to fit classifier parameters. We apply DeepType to the problem of Entity Linking on three standard datasets (i.e. WikiDisamb30, CoNLL (YAGO), TAC KBP 2010) and find that it outperforms all existing solutions by a wide margin, including approaches that rely on a human-designed type system or recent deep learning-based entity embeddings, while explicitly using symbolic information lets it integrate new entities without retraining.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Core: Language Processing', 'In-House provided'),
('2018', 'Houthooft, R. & Chen, R. et Al.', 'Evolved Policy Gradients', 'http://arxiv.org/abs/1802.04821', 'We propose a metalearning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss is parametrized via temporal convolutions over the agent\'s experience. Because this loss is highly flexible in its ability to take into account the agent\'s history, it enables fast task learning. Empirical results show that our evolved policy gradient algorithm (EPG) achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method. We also demonstrate that EPG\'s learned loss can generalize to out-of-distribution test time tasks, and exhibits qualitatively different behavior from other popular metalearning algorithms.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'In-House provided'),
('2018', 'Brundage, M. & Avin, S. et Al.', 'The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation', 'http://arxiv.org/abs/1802.07228', 'This report surveys the landscape of potential security threats from malicious uses of AI, and proposes ways to better forecast, prevent, and mitigate these threats. After analyzing the ways in which AI may influence the threat landscape in the digital, physical, and political domains, we make four high-level recommendations for AI researchers and other stakeholders. We also suggest several promising areas for further research that could expand the portfolio of defenses, or make attacks less effective or harder to execute. Finally, we discuss, but do not conclusively resolve, the long-term equilibrium of attackers and defenders.', 'Society and AI Alignment', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Core: Language Processing', 'In-House provided'),
('2019', 'Stadie, B. & Yang, G. et Al.', 'Some Considerations on Learning to Explore via Meta-Reinforcement Learning', 'http://arxiv.org/abs/1803.01118', 'We consider the problem of exploration in meta reinforcement learning. Two new meta reinforcement learning algorithms are suggested: E-MAML and E-$\\text{RL}^2$. Results are presented on a novel environment we call `Krazy World\' and a set of maze environments. We show E-MAML and E-$\\text{RL}^2$ deliver better performance on tasks where exploration is important.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'In-House provided'),
('2018', 'Nichol, A. & Achiam, J. et Al.', 'On First-Order Meta-Learning Algorithms', 'http://arxiv.org/abs/1803.02999', 'This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'In-House provided'),
('2018', 'Wu, C. & Rajeswaran, A. et Al.', 'Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines', 'http://arxiv.org/abs/1803.07246', 'Policy gradient methods have enjoyed great success in deep reinforcement learning but suffer from high variance of gradient estimates. The high variance problem is particularly exasperated in problems with long horizons or high-dimensional action spaces. To mitigate this issue, we derive a bias-free action-dependent baseline for variance reduction which fully exploits the structural form of the stochastic policy itself and does not make any additional assumptions about the MDP. We demonstrate and quantify the benefit of the action-dependent baseline through both theoretical analysis as well as numerical results, including an analysis of the suboptimality of the optimal state-dependent baseline. The result is a computationally efficient policy gradient algorithm, which scales to high-dimensional control problems, as demonstrated by a synthetic 2000-dimensional target matching task. Our experimental results indicate that action-dependent baselines allow for faster learning on standard reinforcement learning benchmarks and high-dimensional hand manipulation and synthetic tasks. Finally, we show that the general idea of including additional information in baselines for improved variance reduction can be extended to partially observed and multi-agent tasks.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'In-House provided'),
('2018', 'Irving, G. & Christiano, P. et Al.', 'AI safety via debate', 'http://arxiv.org/abs/1805.00899', 'To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier\'s accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.', 'Society and AI Alignment', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2018', 'Grover, A. & Al-Shedivat, M. et Al.', 'Learning Policy Representations in Multiagent Systems', 'http://arxiv.org/abs/1806.06464', 'Modeling agent behavior is central to understanding the emergence of complex phenomena in multiagent systems. Prior work in agent modeling has largely been task-specific and driven by hand-engineering domain-specific prior knowledge. We propose a general learning framework for modeling agent behavior in any multiagent system using only a handful of interaction data. Our framework casts agent modeling as a representation learning problem. Consequently, we construct a novel objective inspired by imitation learning and agent identification and design an algorithm for unsupervised learning of representations of agent policies. We demonstrate empirically the utility of the proposed framework in (i) a challenging high-dimensional competitive environment for continuous control and (ii) a cooperative environment for communication, on supervised predictive tasks, unsupervised clustering, and policy optimization using deep reinforcement learning.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'In-House provided'),
('2018', 'Burda, Y. & Edwards, H. et Al.', 'Large-Scale Study of Curiosity-Driven Learning', 'http://arxiv.org/abs/1808.04355', 'Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github.io/large-scale-curiosity/', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'In-House provided'),
('2018', 'Grathwohl, W. & Chen, R. T. Q. et Al.', 'FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models', 'http://arxiv.org/abs/1810.01367', 'A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson\'s trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'In-House provided'),
('2018', 'Christiano, P. & Shlegeris, B. et Al.', 'Supervising strong learners by amplifying weak experts', 'http://arxiv.org/abs/1810.08575', 'Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'In-House provided'),
('2018', 'Burda, Y. & Edwards, H. et A.', 'Exploration by Random Network Distillation', 'http://arxiv.org/abs/1810.12894', 'We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma\'s Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'In-House provided'),
('2019', 'Lowrey, K. & Rajeswaran, A. et Al.', 'Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control', 'http://arxiv.org/abs/1811.01848', 'We propose a plan online and learn offline (POLO) framework for the setting where an agent, with an internal model, needs to continually act and learn in the world. Our work builds on the synergistic relationship between local model-based control, global value function learning, and exploration. We study how local trajectory optimization can cope with approximation errors in the value function, and can stabilize and accelerate value function learning. Conversely, we also study how approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions. Finally, we also demonstrate how trajectory optimization can be used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation. This exploration is critical for fast and stable learning of the value function. Combining these components enable solutions to complex simulated control tasks, like humanoid locomotion and dexterous in-hand manipulation, in the equivalent of a few minutes of experience in the real world.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'In-House provided'),
('2020', 'Du, Y. & Mordatch, I. et Al.', 'Implicit Generation and Generalization in Energy-Based Models', 'http://arxiv.org/abs/1903.08689', 'Energy based models (EBMs) are appealing due to their generality and simplicity in likelihood modeling, but have been traditionally difficult to train. We present techniques to scale MCMC based EBM training on continuous neural networks, and we show its success on the high-dimensional data domains of ImageNet32x32, ImageNet128x128, CIFAR-10, and robotic hand trajectories, achieving better samples than other likelihood models and nearing the performance of contemporary GAN approaches, while covering all modes of the data. We highlight some unique capabilities of implicit generation such as compositionality and corrupt image reconstruction and inpainting. Finally, we show that EBMs are useful models across a wide variety of tasks, achieving state-of-the-art out-of-distribution classification, adversarially robust classification, state-of-the-art continual online class learning, and coherent long term predicted trajectory rollouts.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'In-House provided'),
('2019', 'Child, R. & Gray, S. et Al.', 'Generating Long Sequences with Sparse Transformers', 'http://arxiv.org/abs/1904.10509', 'Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \\sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'In-House provided'),
('2019', 'Solaiman, I. &  Brundage, Miles; C. et Al.', 'Release Strategies and the Social Impacts of Language Models', 'http://arxiv.org/abs/1908.09203', 'Large language models have a range of beneficial uses: they can assist in prose, poetry, and programming; analyze dataset biases; and more. However, their flexibility and generative capabilities also raise misuse concerns. This report discusses OpenAI\'s work related to the release of its GPT-2 language model. It discusses staged release, which allows time between model releases to conduct risk and benefit analyses as model sizes increased. It also discusses ongoing partnership-based research and provides recommendations for better coordination and responsible publication in AI.', 'Society and AI Responsibility', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Core: Language Processing', 'In-House provided'),
('2020', 'Baker, B. & Kanitscheider, I. et Al.', 'Emergent Tool Use From Multi-Agent Autocurricula', 'http://arxiv.org/abs/1909.07528', 'Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'In-House provided'),
('2020', 'Ziegler, D. M. & Stiennon, N. et Al.', 'Fine-Tuning Language Models from Human Preferences', 'http://arxiv.org/abs/1909.08593', 'Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2020', 'Kaplan, J. & McCandlish, S. et Al.', 'Scaling Laws for Neural Language Models', 'http://arxiv.org/abs/2001.08361', 'We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2020', 'Brown, T. B. & Mann, B. et Al.', 'Language Models are Few-Shot Learners', 'http://arxiv.org/abs/2005.14165', 'Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3\'s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2022', 'Stiennon, N. & Ouyang, L. et Al.', 'Learning to summarize from human feedback', 'http://arxiv.org/abs/2009.01325', 'As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.', 'NLP', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided');
INSERT INTO `CoCreating` (`Year`, `AuthorID`, `Title`, `SourceID`, `Abstract`, `ResearchField`, `DeploymentStatus`, `Publisher`, `Language`, `ComplexityState`, `EvolvementRealm`, `StartingPoint`) VALUES
('2020', 'Polu, S. & Sutskever, I. et Al.', 'Generative Language Modeling for Automated Theorem Proving', 'http://arxiv.org/abs/2009.03393', 'We explore the application of transformer-based language models to automated theorem proving. This work is motivated by the possibility that a major limitation of automated theorem provers compared to humans -- the generation of original mathematical terms -- might be addressable via generation from language models. We present an automated prover and proof assistant, GPT-f, for the Metamath formalization language, and analyze its performance. GPT-f found new short proofs that were accepted into the main Metamath library, which is to our knowledge, the first time a deep-learning based system has contributed proofs that were adopted by a formal mathematics community.', 'NLP', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2021', 'Tamkin, A. & Brundage, M. et Al.', 'Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models', 'http://arxiv.org/abs/2102.02503', 'On October 14th, 2020, researchers from OpenAI, the Stanford Institute for Human-Centered Artificial Intelligence, and other universities convened to discuss open research questions surrounding GPT-3, the largest publicly-disclosed dense language model at the time. The meeting took place under Chatham House Rules. Discussants came from a variety of research backgrounds including computer science, linguistics, philosophy, political science, communications, cyber policy, and more. Broadly, the discussion centered around two main questions: 1) What are the technical capabilities and limitations of large language models? 2) What are the societal effects of widespread use of large language models? Here, we provide a detailed summary of the discussion organized by the two themes above.', 'NLP', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Core: Language Processing', 'In-House provided'),
('2021', 'Chen, M. & Tworek, J. et Al.', 'Evaluating Large Language Models Trained on Code', 'http://arxiv.org/abs/2107.03374', 'We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.', 'NLP', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2022', 'Lin, S. & Hilton, J.et Al.', 'TruthfulQA: Measuring How Models Mimic Human Falsehoods', 'http://arxiv.org/abs/2109.07958', 'We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.', 'Society and AI Alignment', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2021', 'Cobbe, K. & Kosaraju, V. et Al.', 'Training Verifiers to Solve Math Word Problems', 'http://arxiv.org/abs/2110.14168', 'State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.', 'NLP', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2022', 'Nakano, R. & Hilton, J. et Al.', 'WebGPT: Browser-assisted question-answering with human feedback', 'http://arxiv.org/abs/2112.09332', 'We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model\'s answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.', 'NLP', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2022', 'Neelakantan, A. & Xu, T. et Al.', 'Text and Code Embeddings by Contrastive Pre-Training', 'http://arxiv.org/abs/2201.10005', 'Text embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4% and 1.8% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4%, 14.7%, and 10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8% relative improvement over prior best work on code search.', 'NLP', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2022', 'Polu, S. & Han, J. et Al.', 'Formal Mathematics Statement Curriculum Learning', 'http://arxiv.org/abs/2202.01344', 'We explore the use of expert iteration in the context of language modeling applied to formal mathematics. We show that at same compute budget, expert iteration, by which we mean proof search interleaved with learning, dramatically outperforms proof search only. We also observe that when applied to a collection of formal statements of sufficiently varied difficulty, expert iteration is capable of finding and solving a curriculum of increasingly difficult problems, without the need for associated ground-truth proofs. Finally, by applying this expert iteration to a manually curated set of problem statements, we achieve state-of-the-art on the miniF2F benchmark, automatically solving multiple challenging problems drawn from high school olympiads.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2022', 'Ouyang, L. & Wu, J. et Al.', 'Training language models to follow instructions with human feedback', 'http://arxiv.org/abs/2203.02155', 'Making language models bigger does not inherently make them better at following a user\'s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.', 'Society and AI Alignment', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2022', 'Lin, S. & Hilton, J.et Al.', 'Teaching Models to Express Their Uncertainty in Words', 'http://arxiv.org/abs/2205.14334', 'We show that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits. When given a question, the model generates both an answer and a level of confidence (e.g. \"90% confidence\" or \"high confidence\"). These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift, and is sensitive to uncertainty in its own answers, rather than imitating human examples. To our knowledge, this is the first time a model has been shown to express calibrated uncertainty about its own answers in natural language. For testing calibration, we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words (\"verbalized probability\") to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT-3\'s ability to generalize calibration depends on pre-trained latent representations that correlate with epistemic uncertainty over its answers.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2022', 'Saunders, W. & Yeh, C. et Al.', 'Self-critiquing models for assisting human evaluators', 'http://arxiv.org/abs/2206.05802', 'We fine-tune large language models to write natural language critiques (natural language critical comments) using behavioral cloning. On a topic-based summarization task, critiques written by our models help humans find flaws in summaries that they would have otherwise missed. Our models help find naturally occurring flaws in both model and human written summaries, and intentional flaws in summaries written by humans to be deliberately misleading. We study scaling properties of critiquing with both topic-based summarization and synthetic tasks. Larger models write more helpful critiques, and on most tasks, are better at self-critiquing, despite having harder-to-critique outputs. Larger models can also integrate their own self-critiques as feedback, refining their own summaries into better ones. Finally, we motivate and introduce a framework for comparing critiquing ability to generation and discrimination ability. Our measurements suggest that even large models may still have relevant knowledge they cannot or do not articulate as critiques. These results are a proof of concept for using AI-assisted human feedback to scale the supervision of machine learning systems to tasks that are difficult for humans to evaluate directly. We release our training datasets, as well as samples from our critique assistance experiments.', 'NLP', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'In-House provided'),
('2022', 'Lehman, J. & Gordon, J. et Al.', 'Evolution through Large Models', 'http://arxiv.org/abs/2206.08896', 'This paper pursues the insight that large language models (LLMs) trained to generate code can vastly improve the effectiveness of mutation operators applied to programs in genetic programming (GP). Because such LLMs benefit from training data that includes sequential changes and modifications, they can approximate likely changes that humans would make. To highlight the breadth of implications of such evolution through large models (ELM), in the main experiment ELM combined with MAP-Elites generates hundreds of thousands of functional examples of Python programs that output working ambulating robots in the Sodarace domain, which the original LLM had never seen in pre-training. These examples then help to bootstrap training a new conditional language model that can output the right walker for a particular terrain. The ability to bootstrap new models that can output appropriate artifacts for a given context in a domain where zero training data was previously available carries implications for open-endedness, deep learning, and reinforcement learning. These implications are explored here in depth in the hope of inspiring new directions of research now opened up by ELM.', 'NLP', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Core: Language Processing', 'In-House provided'),
('2022', 'Khlaaf, H. & Mishkin, P. et Al.', 'A Hazard Analysis Framework for Code Synthesis Large Language Models', 'http://arxiv.org/abs/2207.14157', 'Codex, a large language model (LLM) trained on a variety of codebases, exceeds the previous state of the art in its capacity to synthesize and generate code. Although Codex provides a plethora of benefits, models that may generate code on such scale have significant limitations, alignment problems, the potential to be misused, and the possibility to increase the rate of progress in technical fields that may themselves have destabilizing impacts or have misuse potential. Yet such safety impacts are not yet known or remain to be explored. In this paper, we outline a hazard analysis framework constructed at OpenAI to uncover hazards or safety risks that the deployment of models like Codex may impose technically, socially, politically, and economically. The analysis is informed by a novel evaluation framework that determines the capacity of advanced code generation techniques against the complexity and expressivity of specification prompts, and their capability to understand and execute them relative to human ability.', 'Safety and AI Alignment ', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2022', 'Bavarian, M. & Jun, H. et Al.', 'Efficient Training of Language Models to Fill in the Middle', 'http://arxiv.org/abs/2207.14255', 'We show that autoregressive language models can learn to infill text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end. While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales. Given the usefulness, simplicity, and efficiency of training models to fill-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default. To this end, we run a series of ablations on key hyperparameters, such as the data transformation frequency, the structure of the transformation, and the method of selecting the infill span. We use these ablations to prescribe strong default settings and best practices to train FIM models. We have released our best infilling model trained with best practices in our API, and release our infilling benchmarks to aid future research.', 'NLP', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2022', 'Gao, L. & Schulman, J. et Al', 'Scaling Laws for Reward Model Overoptimization', 'http://arxiv.org/abs/2210.10760', 'In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart\'s law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed \"gold-standard\" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-$n$ sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'In-House provided'),
('2023', 'OpenAI', 'GPT-4 Technical Report', 'http://arxiv.org/abs/2303.08774', 'We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4\'s performance based on models trained with no more than 1/1,000th the compute of GPT-4.', 'Neural Networks and AI Modeling', 'Post-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing\n\n', 'In-House provided'),
('2023', 'Eloundou et Al.  ', 'GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models', 'http://arxiv.org/abs/2303.10130', 'We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications.', 'Society and AI Alignment', 'Post-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing\n\n', 'In-House provided'),
('2010', 'Glorot and Bengio', 'Understanding the difficulty of training deep feedforward neural networks', 'http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf', 'Whereas before 2006 it appears that deep multi-\nlayer neural networks were not successfully\ntrained, since then several algorithms have been\nshown to successfully train them, with experi-\nmental results showing the superiority of deeper\nvs less deep architectures. All these experimen-\ntal results were obtained with new initialization\nor training mechanisms. Our objective here is to\nunderstand better why standard gradient descent\nfrom random initialization is doing so poorly\nwith deep neural networks, to better understand\nthese recent relative successes and help design\nbetter algorithms in the future. We first observe\nthe influence of the non-linear activations func-\ntions. We find that the logistic sigmoid activation\nis unsuited for deep networks with random ini-\ntialization because of its mean value, which can\ndrive especially the top hidden layer into satu-\nration. Surprisingly, we find that saturated units\ncan move out of saturation by themselves, albeit\nslowly, and explaining the plateaus sometimes\nseen when training neural networks. We find that\na new non-linearity that saturates less can often\nbe beneficial. Finally, we study how activations\nand gradients vary across layers and during train-\ning, with the idea that training may be more dif-\nficult when the singular values of the Jacobian\nassociated with each layer are far from 1. Based\non these considerations, we propose a new ini-\ntialization scheme that brings substantially faster\nconvergence.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'Academic Research'),
('2013', 'Ioffe and Szegedy', 'Distributed Representations of Words and Phrases and their Compositionality', 'https://arxiv.org/abs/1310.4546%0A', 'The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible. \n\n', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'Academic Research'),
('2013', 'Mnih et al.', 'Playing Atari with Deep Reinforcement Learning', 'https://arxiv.org/abs/1312.5602', 'We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them. ', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'Academic Research'),
('2014', 'Sutskever et al.', 'Sequence to Sequence Learning with Neural Networks', 'https://arxiv.org/abs/1409.3215', 'Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT\'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM\'s BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM\'s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier. ', 'NLP', 'Pre-Deployment', 'Google', 'English', 'Foundational', 'Core: Language Processing', 'In-House provided'),
('2014', 'Kingma and Ba', 'Adam: A Method for Stochastic Optimization', 'https://arxiv.org/abs/1412.6980', 'We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm. ', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2016', 'Zoph and Le', 'Neural Architecture Search with Reinforcement Learning', 'https://arxiv.org/abs/1611.01578', 'Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214. ', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'Academic Research'),
('2017', 'Pathak et al.', 'Curiosity-driven Exploration by Self-supervised Prediction', 'https://arxiv.org/abs/1705.05363', 'In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent\'s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at this https URL', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'Academic Research'),
('2017', 'Vaswani et al.', 'Attention is All You Need', 'https://arxiv.org/abs/1706.03762', 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. ', 'NLP', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Core: Language Processing', 'In-House provided'),
('2017', 'Schulman et al.', 'Proximal Policy Optimization Algorithms', 'https://arxiv.org/abs/1707.06347', 'We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time. ', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'Academic Research'),
('2017', ' Bellemare et Al.', 'A Distributional Perspective on Reinforcement Learning', 'https://arxiv.org/abs/1707.06887', 'In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman\'s equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting. ', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'Academic Research'),
('2018', 'Juliani et al.', 'Unity: A General Platform for Intelligent Agents', 'https://arxiv.org/abs/1809.02627', 'Recent advances in artificial intelligence have been driven by the presence of increasingly realistic and complex simulated environments. However, many of the existing environments provide either unrealistic visuals, inaccurate physics, low task complexity, restricted agent perspective, or a limited capacity for interaction among artificial agents. Furthermore, many platforms lack the ability to flexibly configure the simulation, making the simulated environment a black-box from the perspective of the learning system. In this work, we propose a novel taxonomy of existing simulation platforms and discuss the highest level class of general platforms which enable the development of learning environments that are rich in visual, physical, task, and social complexity. We argue that modern game engines are uniquely suited to act as general platforms and as a case study examine the Unity engine and open source Unity ML-Agents Toolkit. We then survey the research enabled by Unity and the Unity ML-Agents Toolkit, discussing the kinds of research a flexible, interactive and easily configurable general platform can facilitate. ', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'Academic Research'),
('2021', 'Lee at Al.', 'Incorporating Behavioral Constraints in Online AI Systems', 'https://arxiv.org/abs/1809.05720v1', 'AI systems that learn through reward feedback about the actions they take are increasingly deployed in domains that have significant impact on our daily life. However, in many cases the online rewards should not be the only guiding criteria, as there are additional constraints and/or priorities imposed by regulations, values, preferences, or ethical principles. We detail a novel online agent that learns a set of behavioral constraints by observation and uses these learned constraints as a guide when making decisions in an online setting while still being reactive to reward feedback. To define this agent, we propose to adopt a novel extension to the classical contextual multi-armed bandit setting and we provide a new algorithm called Behavior Constrained Thompson Sampling (BCTS) that allows for online learning while obeying exogenous constraints. Our agent learns a constrained policy that implements the observed behavioral constraints demonstrated by a teacher agent, and then uses this constrained policy to guide the reward-based online exploration and exploitation. We characterize the upper bound on the expected regret of the contextual bandit algorithm that underlies our agent and provide a case study with real world data in two application domains. Our experiments show that the designed agent is able to act within the set of behavior constraints without significantly degrading its overall reward performance. ', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'Academic Research'),
('2018', 'Devlin et al.', 'BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding', 'https://arxiv.org/abs/1810.04805', 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n    BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement). \n', 'NLP', 'Pre-Deployment', 'Google', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2021', 'Kaparthy', 'Scaling Laws for Neural Language Models', 'https://arxiv.org/abs/2001.08361', 'We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence. ', 'NLP', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'Academic Research'),
('2022', 'Radford, A. & Kim, J. W. et Al. ', 'Robust Speech Recognition via Large-Scale Weak Supervision', 'https://arxiv.org/abs/2212.04356', 'We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.', 'NLP', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing\n\n', 'In-House provided'),
('2017 ', 'Usunier et al.', 'Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games', 'https://arxiv.org/pdf/1703.10069v1.pdf', 'Real-world artificial intelligence (AI) applications often require multiple agents\nto work in a collaborative effort. Efficient learning for intra-agent communication\nand coordination is an indispensable step towards general AI. In this paper, we take\nStarCraft combat game as the test scenario, where the task is to coordinate multiple\nagents as a team to defeat their enemies. To maintain a scalable yet effective\ncommunication protocol, we introduce a multiagent bidirectionally-coordinated\nnetwork (BiCNet [’bIknet]) with a vectorised extension of actor-critic formulation.\nWe show that BiCNet can handle different types of combats under diverse terrains\nwith arbitrary numbers of AI agents for both sides. Our analysis demonstrates\nthat without any supervisions such as human demonstrations or labelled data,\nBiCNet could learn various types of coordination strategies that is similar to these\nof experienced game players. Moreover, BiCNet is easily adaptable to the tasks\nwith heterogeneous agents. In our experiments, we evaluate our approach against\nmultiple baselines under different scenarios; it shows state-of-the-art performance,\nand possesses potential values for large-scale real-world applications.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'Academic Research'),
('2018 ', 'Houthooft et al.', 'Evolved Policy Gradients', 'https://arxiv.org/pdf/1802.04821.pdf', 'We propose a metalearning approach for learning\ngradient-based reinforcement learning (RL) algo-\nrithms. The idea is to evolve a differentiable loss\nfunction, such that an agent, which optimizes its\npolicy to minimize this loss, will achieve high re-\nwards. The loss is parametrized via temporal con-\nvolutions over the agent’s experience. Because\nthis loss is highly flexible in its ability to take\ninto account the agent’s history, it enables fast\ntask learning. Empirical results show that our\nevolved policy gradient algorithm (EPG) achieves\nfaster learning on several randomized environ-\nments compared to an off-the-shelf policy gra-\ndient method. We also demonstrate that EPG’s\nlearned loss can generalize to out-of-distribution\ntest time tasks, and exhibits qualitatively different\nbehavior from other popular metalearning algo-\nrithms.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'Academic Research'),
('2019', 'Radford et al', 'GPT-2: Language Models are Unsupervised Multitask Learners', 'https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf', 'Natural language processing tasks, such as ques-\ntion answering, machine translation, reading com-\nprehension, and summarization, are typically\napproached with supervised learning on task-\nspecific datasets. We demonstrate that language\nmodels begin to learn these tasks without any ex-\nplicit supervision when trained on a new dataset\nof millions of webpages called WebText. When\nconditioned on a document plus questions, the an-\nswers generated by the language model reach 55\nF1 on the CoQA dataset - matching or exceeding\nthe performance of 3 out of 4 baseline systems\nwithout using the 127,000+ training examples.\nThe capacity of the language model is essential\nto the success of zero-shot task transfer and in-\ncreasing it improves performance in a log-linear\nfashion across tasks. Our largest model, GPT-2,\nis a 1.5B parameter Transformer that achieves\nstate of the art results on 7 out of 8 tested lan-\nguage modeling datasets in a zero-shot setting\nbut still underfits WebText. Samples from the\nmodel reflect these improvements and contain co-\nherent paragraphs of text. These findings suggest\na promising path towards building language pro-\ncessing systems which learn to perform tasks from\ntheir naturally occurring demonstrations', 'NLP', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2008', 'Busoniu et al.', 'Cooperative Multi-Agent Learning: The State of the Art', 'https://cs.gmu.edu/~eclab/papers/panait05cooperative.pdf', 'Cooperative multi-agent systems are ones in which several agents attempt, through their interaction, to jointly\nsolve tasks or to maximize utility. Due to the interactions among the agents, multi-agent problem complexity can\nrise rapidly with the number of agents or their behavioral sophistication. The challenge this presents to the task\nof programming solutions to multi-agent systems problems has spawned increasing interest in machine learning\ntechniques to automate the search and optimization process.\nWe provide a broad survey of the cooperative multi-agent learning literature. Previous surveys of this area have\nlargely focused on issues common to specific subareas (for example, reinforcement learning or robotics). In this\nsurvey we attempt to draw from multi-agent learning work in a spectrum of areas, including reinforcement learning,\nevolutionary computation, game theory, complex systems, agent modeling, and robotics.\nWe find that this broad view leads to a division of the work into two categories, each with its own special is-\nsues: applying a single learner to discover joint solutions to multi-agent problems (team learning), or using multiple\nsimultaneous learners, often one per agent (concurrent learning). Additionally, we discuss direct and indirect commu-\nnication in connection with learning, plus open issues in task decomposition, scalability, and adaptive dynamics. We\nconclude with a presentation of multi-agent learning problem domains, and a list of multi-agent learning resources', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'Academic Research');
INSERT INTO `CoCreating` (`Year`, `AuthorID`, `Title`, `SourceID`, `Abstract`, `ResearchField`, `DeploymentStatus`, `Publisher`, `Language`, `ComplexityState`, `EvolvementRealm`, `StartingPoint`) VALUES
('2015', 'Mnih et al.', 'Human-level control through deep reinforcement learning', 'https://daiwk.github.io/assets/dqn.pdf', 'The theory of reinforcement learning provides a normative account1\n,\ndeeply rooted in psychological2 and neuroscientific3 perspectives on\nanimal behaviour, of how agents may optimize their control of an\nenvironment. To use reinforcement learning successfully in situations\napproaching real-world complexity, however, agents are confronted\nwith a difficult task: they must derive efficient representations of the\nenvironment from high-dimensional sensory inputs, and use these\nto generalize past experience to new situations. Remarkably, humans\nand other animals seem to solve this problem through a harmonious\ncombination of reinforcement learning and hierarchical sensory pro-\ncessing systems4,5\n, the former evidenced by a wealth of neural data\nrevealing notable parallels between the phasic signals emitted by dopa-\nminergic neurons and temporal difference reinforcement learning\nalgorithms3\n. While reinforcement learning agents have achieved some\nsuccesses in a variety of domains6–8\n, their applicability has previously\nbeen limited to domains in which useful features can be handcrafted,\nor to domains with fully observed, low-dimensional state spaces.\nHere we use recent advances in training deep neural networks9–11 to\ndevelop a novel artificial agent, termed a deep Q-network, that can\nlearn successful policies directly from high-dimensional sensory inputs\nusing end-to-end reinforcement learning. We tested this agent on\nthe challenging domain of classic Atari 2600 games 12\n. We demon-\nstrate that the deep Q-network agent, receiving only the pixels and\nthe game score as inputs, was able to surpass the performance of all\nprevious algorithms and achieve a level comparable to that of a pro-\nfessional human games tester across a set of 49 games, using the same\nalgorithm, network architecture and hyperparameters. This work\nbridges the divide between high-dimensional sensory inputs and\nactions, resulting in the first artificial agent that is capable of learn-\ning to excel at a diverse array of challenging tasks.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'Academic Research'),
('2019', 'Irving, G. & Askell, A. et Al.', 'AI Safety Needs Social Scientists', 'https://distill.pub/2019/safety-needs-social-scientists', 'If we want to train AI to do what humans want, we need to study humans.', 'Society and AI Alignment', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Core: Language Processing', 'In-House provided'),
('2012', 'Hinton et Al.', 'ImageNet Classification with Deep Convolutional Neural Networks', 'https://dl.acm.org/doi/10.1145/3065386', 'We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'Academic Research'),
('2016', 'Johnson et al.', 'The Malmo Platform for Artificial Intelligence Experimentation', 'https://dl.acm.org/doi/10.5555/3061053.3061259', '\n\nWe present Project Malmo - an AI experimentation platform built on top of the popular computer game Minecraft, and designed to support fundamental research in artificial intelligence. As the AI research community pushes for artificial general intelligence (AGI), experimentation platforms are needed that support the development of flexible agents that learn to solve diverse tasks in complex environments. Minecraft is an ideal foundation for such a platform, as it exposes agents to complex 3D worlds, coupled with infinitely varied game-play.\n\nProject Malmo provides a sophisticated abstraction layer on top of Minecraft that supports a wide range of experimentation scenarios, ranging from navigation and survival to collaboration and problem solving tasks. In this demo we present the Malmo platform and its capabilities. The platform is publicly released as open source software at IJCAI, to support openness and collaboration in AI research.\n', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'Academic Research'),
('2019', 'Raffel et al.', 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer', 'https://dl.acm.org/doi/10.5555/3455716.3455856', 'Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.', 'NLP', 'Pre-Deployment', 'Google', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2003 ', 'Bengio et al', 'A Neural Probabilistic Language Model', 'https://dl.acm.org/doi/10.5555/944919.944966', 'A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.', 'NLP', 'Pre-Deployment', 'AT&T Labs', 'English', 'Foundational', 'Core: Language Processing', 'In-House provided'),
('2014', 'Srivastava et al.', 'Dropout: A Simple Way to Prevent Neural Networks from Overfitting', 'https://dl.acm.org/doi/abs/10.5555/2627435.2670313', 'Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'Academic Research'),
('2007', 'Bengio et al.', 'Greedy Layer-Wise Training of Deep Networks', 'https://ieeexplore.ieee.org/document/6287632', 'Complexity theory of circuits strongly suggests that deep architectures can be much more ef cient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'Academic Research'),
('2016', 'OpenAI', 'Generative models', 'https://openai.com/research/generative-models', 'This post describes four projects that share a common theme of enhancing or using generative models, a branch of unsupervised learning techniques in machine learning. In addition to describing our work, this post will tell you a bit more about generative models: what they are, why they are important, and where they might be going.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Core: Language Processing', 'In-House provided'),
('2016', 'OpenAI', 'Infrastructure for deep learning', 'https://openai.com/research/infrastructure-for-deep-learning', 'Deep learning is an empirical science, and the quality of a group’s infrastructure is a multiplier on progress. Fortunately, today’s open-source ecosystem makes it possible for anyone to build great deep learning infrastructure.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Core: Language Processing', 'In-House provided'),
('2022', 'OpenAI', 'Lessons learned on language model safety and misuse', 'https://openai.com/research/language-model-safety-and-misuse', 'We describe our latest thinking in the hope of helping other AI developers address safety and misuse of deployed models.', 'Society and AI Alignment', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2018', 'OpenAI', 'Learning Montezuma’s Revenge from a single demonstration', 'https://openai.com/research/learning-montezumas-revenge-from-a-single-demonstration', 'We’ve trained an agent to achieve a high score of 74,500 on Montezuma’s Revenge from a single human demonstration, better than any previously published result. Our algorithm is simple: the agent plays a sequence of games starting from carefully chosen states from the demonstration, and learns from them by optimizing the game score using PPO, the same reinforcement learning algorithm that underpins OpenAI Five.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'In-House provided'),
('2017', 'OpenAI', 'Learning to communicate', 'https://openai.com/research/learning-to-communicate', 'In this post we’ll outline new OpenAI research in which agents develop their own language.', 'NLP', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Core: Language Processing', 'In-House provided'),
('2022', 'OpenAI', 'Measuring Goodhart’s law', 'https://openai.com/research/measuring-goodharts-law', 'Goodhart’s law famously says: “When a measure becomes a target, it ceases to be a good measure.” Although originally from economics, it’s something we have to grapple with at OpenAI when figuring out how to optimize objectives that are difficult or costly to measure.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'In-House provided'),
('2022', 'OpenAI', 'Techniques for training large neural networks', 'https://openai.com/research/techniques-for-training-large-neural-networks', 'Large neural networks are at the core of many recent advances in AI, but training them is a difficult engineering and research challenge which requires orchestrating a cluster of GPUs to perform a single synchronized calculation.', 'NLP', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'In-House provided'),
('2006', 'Hinton et al.', 'A Fast Learning Algorithm for Deep Belief Nets', 'https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf', 'We show how to use “complementary priors” to\neliminate the explaining away effects that make\ninference difficult in densely-connected belief\nnets that have many hidden layers. Using com-\nplementary priors, we derive a fast, greedy algo-\nrithm that can learn deep, directed belief networks\none layer at a time, provided the top two lay-\ners form an undirected associative memory. The\nfast, greedy algorithm is used to initialize a slower\nlearning procedure that fine-tunes the weights us-\ning a contrastive version of the wake-sleep algo-\nrithm. After fine-tuning, a network with three\nhidden layers forms a very good generative model\nof the joint distribution of handwritten digit im-\nages and their labels. This generative model gives\nbetter digit classification than the best discrimi-\nnative learning algorithms. The low-dimensional\nmanifolds on which the digits lie are modelled by\nlong ravines in the free-energy landscape of the\ntop-level associative memory and it is easy to ex-\nplore these ravines by using the directed connec-\ntions to display what the associative memory has\nin mind.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'Academic Research'),
('2015', 'Hinton et Al.', 'Deep Learning', 'https://www.nature.com/articles/nature14539', 'Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'Academic Research'),
('2016', 'Silver et al.', 'Mastering the game of Go with deep neural networks and tree search', 'https://www.nature.com/articles/nature16961', 'The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'Academic Research'),
('2006', 'Hinton et Al.', 'Reducing the dimensionality of data with neural networks', 'https://www.science.org/doi/10.1126/science.1127647', 'High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.', 'Neural Networks and AI Modeling', 'Pre-Deployment', 'OpenAI', 'English', 'Foundational', 'Periphery: Intrafields', 'Academic Research'),
('2018', ' Silver et al.', 'A General Reinforcement Learning Algorithm that Masters Chess, Shogi, and Go through Self-Play', 'https://www.science.org/doi/10.1126/science.aar6404', 'Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver et al. developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system.', 'Training Methodologies ', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Periphery: Intrafields', 'Academic Research'),
('2020', 'Ettinger et Al.', 'Does BERT Need to Understand Language?', 'https://www.techtarget.com/searchenterpriseai/definition/BERT-language-model', 'BERT is an open source machine learning framework for natural language processing (NLP). BERT is designed to help computers understand the meaning of ambiguous language in text by using surrounding text to establish context. The BERT framework was pre-trained using text from Wikipedia and can be fine-tuned with question and answer datasets.\n\nBERT, which stands for Bidirectional Encoder Representations from Transformers, is based on Transformers, a deep learning model in which every output element is connected to every input element, and the weightings between them are dynamically calculated based upon their connection. (In NLP, this process is called attention.)\n\nHistorically, language models could only read text input sequentially -- either left-to-right or right-to-left -- but couldn\'t do both at the same time. BERT is different because it is designed to read in both directions at once. This capability, enabled by the introduction of Transformers, is known as bidirectionality. \n\nUsing this bidirectional capability, BERT is pre-trained on two different, but related, NLP tasks: Masked Language Modeling and Next Sentence Prediction.\n\nThe objective of Masked Language Model (MLM) training is to hide a word in a sentence and then have the program predict what word has been hidden (masked) based on the hidden word\'s context. The objective of Next Sentence Prediction training is to have the program predict whether two given sentences have a logical, sequential connection or whether their relationship is simply random.', 'NLP', 'Pre-Deployment', 'OpenAI', 'English', 'Advanced', 'Core: Language Processing', 'Academic Research');

--
-- Indizes der exportierten Tabellen
--

--
-- Indizes für die Tabelle `CoCreating`
--
ALTER TABLE `CoCreating`
  ADD PRIMARY KEY (`SourceID`);
COMMIT;

/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
